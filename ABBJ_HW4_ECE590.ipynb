{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 590, Fall 2019 \n",
    "## Problem Set 4\n",
    "* ### __Important :__  You are not allowed to use built-in optimizers from Pytorch or any other Python Deep Learning environment. \n",
    "\n",
    "## Full name: Ana B. Barcenas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (First-order Optimization Methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Download and prepare the MNIST data set \n",
    "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract target and features of the dataset and store them as tensors\n",
    "N_train = 60000\n",
    "N_test = 10000\n",
    "\n",
    "train_X = train_dataset.data.reshape(N_train, 28*28).float()\n",
    "train_y = torch.zeros(N_train, 10).scatter_(1, train_dataset.targets.unsqueeze(1), 1)\n",
    "test_X = test_dataset.data.reshape(N_test, 28*28).float()\n",
    "test_y = torch.zeros(N_test, 10).scatter_(1, test_dataset.targets.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_MNIST():\n",
    "    \n",
    "    def __init__(self, optimizer, eta=0.0001, mom_beta=0.9, nag_beta=0.95, lam=0.1, \n",
    "                 batch_size=500, n_iters=5, verbose=0):\n",
    "        \n",
    "        self.train_X = train_dataset.data.reshape(N_train, 28*28).float()\n",
    "        self.train_y = torch.zeros(N_train, 10).scatter_(1, train_dataset.targets.unsqueeze(1), 1)\n",
    "        self.test_X = test_dataset.data.reshape(N_test, 28*28).float()\n",
    "        self.test_y = torch.zeros(N_test, 10).scatter_(1, test_dataset.targets.unsqueeze(1), 1)\n",
    "        self.train_target = train_dataset.targets\n",
    "        self.test_target = test_dataset.targets\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.eta = eta\n",
    "        self.lam = lam\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iters = n_iters\n",
    "        self.verbose = verbose\n",
    "        self.W = torch.randn(28*28, 10, requires_grad=True)\n",
    "        \n",
    "        if optimizer == \"Momentum\":\n",
    "            self.mom_beta = mom_beta\n",
    "            self.pre_momentum = torch.zeros_like(self.W)\n",
    "            \n",
    "        if optimizer == \"NAG\":\n",
    "            self.nag_beta = nag_beta\n",
    "            self.pre_nag_beta = torch.zeros_like(self.W)\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        for k in range(self.n_iters):\n",
    "            b_k = np.random.choice(N_train, self.batch_size, replace=True)\n",
    "            eval(\"self.\"+self.optimizer+\"(b_k)\")\n",
    "            if self.verbose == 1:\n",
    "                self.print_result(k)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self):\n",
    "        y_pred = self.train_X.mm(self.W)\n",
    "        test_acc = (self.test_target == self.test_X.mm(self.W).max(dim=1).indices).sum().item() / N_test\n",
    "        return y_pred, test_acc\n",
    "    \n",
    "    \n",
    "    def SGD(self, b_k):\n",
    "        for k in range(len(b_k)):\n",
    "            i_k = np.random.choice(b_k, 1).item()\n",
    "            y_pred = self.train_X[i_k].unsqueeze(0).mm(self.W)\n",
    "            \n",
    "            loss1 = (-self.train_y[i_k] * torch.log_softmax(y_pred, dim=1).squeeze()).sum()\n",
    "            loss2 = self.lam * self.W.norm(p=2)\n",
    "            total_loss = loss1 + loss2\n",
    "            \n",
    "            total_loss.backward()\n",
    "            with torch.no_grad():\n",
    "                self.W -= self.eta * self.W.grad\n",
    "                self.W.grad.zero_()\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def Momentum(self, b_k):\n",
    "        for k in range(len(b_k)):\n",
    "            i_k = np.random.choice(b_k, 1).item()\n",
    "            y_pred = self.train_X[i_k].unsqueeze(0).mm(self.W)\n",
    "            loss1 = (-self.train_y[i_k] * torch.log_softmax(y_pred, dim=1).squeeze()).sum()\n",
    "            loss2 = self.lam * self.W.norm(p=2)\n",
    "            loss = loss1 + loss2\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                self.pre_momentum = self.mom_beta * self.pre_momentum + self.W.grad\n",
    "                self.W -= self.eta * self.pre_momentum\n",
    "                self.W.grad.zero_()\n",
    "        pass\n",
    "    \n",
    "              \n",
    "\n",
    "\n",
    "    def results(self, k=None):\n",
    "        train_pred = self.train_X.mm(self.W)\n",
    "        train_accuracy = (self.train_target == train_pred.max(dim=1).indices).sum().item() / N_train\n",
    "        test_pred = self.test_X.mm(self.W)\n",
    "        test_accuracy = (self.test_target == test_pred.max(dim=1).indices).sum().item() / N_test\n",
    "        if k:\n",
    "            print(\"Train Accuracy: %.2f%%\" % (100*train_accuracy))\n",
    "            print(\"Test Accuracy: %.2f%%\" % (100*test_accuracy))\n",
    "        else:\n",
    "            print(\"Train Accuracy: %.2f%%   Test Accuracy: %.2f%%\" % (100*train_accuracy, 100*test_accuracy))\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent:\n",
      "Train Accuracy: 86.63%   Test Accuracy: 86.81%\n",
      "Momentum:\n",
      "Train Accuracy: 87.08%   Test Accuracy: 87.30%\n"
     ]
    }
   ],
   "source": [
    "# Let's get some results using the methods coded above:\n",
    "\n",
    "batch_size=500\n",
    "n_iters=120\n",
    "\n",
    "#### SGD ####\n",
    "print('Stochastic Gradient Descent:')\n",
    "sgd = logistic_MNIST(optimizer=\"SGD\", batch_size=batch_size, n_iters=n_iters).fit()\n",
    "sgd.results()\n",
    "\n",
    "#### Momentum ####\n",
    "print('Momentum:')\n",
    "momentum = logistic_MNIST(optimizer=\"Momentum\", batch_size=batch_size, n_iters=n_iters).fit()\n",
    "momentum.results()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (Newtonâ€™s Method for Non-linear Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian matrix for x_1 = 1 and x_2 = 1:\n",
      "[[ 49.85777662 147.83997803]\n",
      " [147.83997803 445.7241498 ]]\n"
     ]
    }
   ],
   "source": [
    "# Objective function\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def obj_func(x_1, x_2):\n",
    "    ''' This function takes x_1 and x_2 as single scalars\n",
    "    and returns the output of substituting the values into the objective function'''\n",
    "    output = math.exp(x_1 + (3*x_2) - 0.1) + math.exp(x_1 - (3*x_2) - 0.1) + math.exp(- x_1 - 0.1)\n",
    "    return output\n",
    "\n",
    "\n",
    "##### HESSIAN MATRIX ######\n",
    "\n",
    "def hessian(x_1, x_2):\n",
    "    ''' This function takes x_1 and x_2 as single scalars\n",
    "    and computes its hessian matrix'''\n",
    "    d_x1_x1 = math.exp(x_1 + (3*x_2) - 0.1) + math.exp(x_1 - (3*x_2) - 0.1) + math.exp(- x_1 - 0.1)\n",
    "    d_x1_x2 = 3*math.exp(x_1 + (3*x_2) - 0.1) - 3*math.exp(x_1 - (3*x_2) - 0.1)\n",
    "    d_x2_x1 = 3*math.exp(x_1 + (3*x_2) - 0.1) - 3*math.exp(x_1 - (3*x_2) - 0.1)\n",
    "    d_x2_x2 = 9*math.exp(x_1 + (3*x_2) - 0.1) + 9*math.exp(x_1 - (3*x_2) - 0.1)   \n",
    "    hess = np.array([[d_x1_x1, d_x1_x2],[d_x2_x1, d_x2_x2]])\n",
    "    return hess\n",
    "\n",
    "def differentiation(x_1, x_2):\n",
    "    ''' This function takes x_1 and x_2 as single scalars\n",
    "    and computes its hessian matrix'''\n",
    "    d_x1 = math.exp(x_1 + (3*x_2) - 0.1) + math.exp(x_1 - (3*x_2) - 0.1) - math.exp(- x_1 - 0.1)\n",
    "    d_x2 = 3*math.exp(x_1 + (3*x_2) - 0.1) - 3*math.exp(x_1 - (3*x_2) - 0.1)  \n",
    "    diff = np.array([d_x1, d_x2])\n",
    "    return diff\n",
    "\n",
    "\n",
    "print('Hessian matrix for x_1 = 1 and x_2 = 1:')\n",
    "print(hessian(1,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6123757974367656\n"
     ]
    }
   ],
   "source": [
    "##### A) ######\n",
    "\n",
    "def newton_meth(step_size, x_1, x_2):\n",
    "    X = np.array([x_1, x_2])\n",
    "    X = X - step_size*np.matmul(np.linalg.inv(hessian(x_1, x_2)),differentiation(x_1, x_2))\n",
    "    return X\n",
    "\n",
    "# Define the starting point X\n",
    "x_1 = 1\n",
    "x_2 = 1\n",
    "X = np.array([x_1, x_2])\n",
    "\n",
    "# Let's run 10,000 iterations and set up a stopping criteria comparing the change in losses after each iterarion\n",
    "for t in range(1,10000):\n",
    "    # Step size\n",
    "    step_size = 0.01\n",
    "    loss = obj_func(X[0], X[1])\n",
    "    X = newton_meth(step_size, X[0], X[1])\n",
    "    # Stopping criteria\n",
    "    if loss - obj_func(X[0], X[1]) < 0.001:\n",
    "        loss = obj_func(X[0], X[1])\n",
    "        break\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6049802357182354\n"
     ]
    }
   ],
   "source": [
    "##### B) ######\n",
    "\n",
    "## I will only update the values of the diagonal and keeping the other values in the Hessian as zeros:\n",
    "\n",
    "for t in range(1,10000):\n",
    "    hess = hessian(X[0], X[1])\n",
    "    hess[0,1] = 0\n",
    "    hess[1,0] = 0\n",
    "    \n",
    "    # Step size\n",
    "    step_size = 0.01\n",
    "    loss = obj_func(X[0], X[1])\n",
    "    X = X - step_size*np.matmul(np.linalg.inv(hess),differentiation(X[0], X[1]))\n",
    "    # Stopping criteria\n",
    "    if loss - obj_func(X[0], X[1]) < 0.001:\n",
    "        loss = obj_func(X[0], X[1])\n",
    "        break\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Logistic Regression using Newtonâ€™s Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Breast cancer dataset preparation\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read csv file \n",
    "df = pd.read_csv('breast_cancer.csv')\n",
    "\n",
    "# extract the 'diagnosis' column as your targets \n",
    "# and convert the entries of targets to 0/1 \n",
    "targets = pd.get_dummies(df.diagnosis).M\n",
    "\n",
    "# extract your features data\n",
    "data = df[[ 'radius_mean', 'texture_mean', 'smoothness_mean',\n",
    "       'compactness_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "       'radius_se', 'texture_se', 'smoothness_se', 'compactness_se',\n",
    "        'symmetry_se', 'fractal_dimension_se']]\n",
    "\n",
    "# train/test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.3, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the sigmoid function\n",
    "def sigmoid(a):\n",
    "    return 1/(1+np.exp(-a))\n",
    "\n",
    "\n",
    "# Compute the differentiation per row\n",
    "def differentiation_row(X,y,w):\n",
    "    sigm = sigmoid(np.matmul(np.transpose(w),X))\n",
    "    row_1o = (sigm - y)*X\n",
    "    row_2o = sigm*(1-sigm)*np.matmul(X.reshape(-1,1),np.transpose(X).reshape(1,-1))    \n",
    "    return row_1o, row_2o\n",
    "\n",
    "# Compute differentiation per term\n",
    "def differentiation(X,y,w):\n",
    "    first_o = np.zeros(X.shape[1])\n",
    "    second_o = np.zeros((X.shape[1],X.shape[1]))  \n",
    "    for i in range(X.shape[0]):\n",
    "        row_1o, row_2o = differentiation_row(np.array(X.iloc[i]),np.array(y.iloc[i]),w)\n",
    "        first_o_ = first_o + row_1o\n",
    "        second_o_ = second_o + row_2o\n",
    "    return first_o_, second_o_\n",
    "\n",
    "# Output of the objective function\n",
    "def obj_func(X,y,w):\n",
    "    output = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        sigm = sigmoid(np.matmul(np.transpose(w),np.array(X.iloc[i])))\n",
    "        row_val = (- np.array(y.iloc[i]) * np.log(sigm)) - ((1-np.array(y.iloc[i])) * np.log(1-sigm))\n",
    "        output += row_val\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anabelen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/anabelen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/anabelen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimized loss (Hessian): nan\n"
     ]
    }
   ],
   "source": [
    "#### A) Hessian Matrix ####\n",
    "\n",
    "# Define random weights\n",
    "w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "for i in range(1,1000):\n",
    "    output, hess = differentiation(X_train,y_train,w)\n",
    "    step_size = 0.01\n",
    "    \n",
    "    loss = obj_func(X_train,y_train,w)\n",
    "    w = w - step_size*np.matmul(output,np.linalg.inv(hm)) \n",
    "    #stopping criteria\n",
    "    if loss - obj_func(X_train,y_train,w) < 0.01:\n",
    "        loss = obj_func(X_train,y_train,w)\n",
    "        break\n",
    "        \n",
    "print(\"Minimized loss (Hessian):\",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anabelen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/anabelen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/anabelen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimized loss (Hessian diagonal approximation): nan\n"
     ]
    }
   ],
   "source": [
    "#### A) Hessian diagonal approximation ####\n",
    "\n",
    "# Define random weights\n",
    "w = np.random.rand(X_train.shape[1])\n",
    "\n",
    "for i in range(1,1000):\n",
    "    output, hess = differentiation(X_train,y_train,w)\n",
    "    hess[0,1] = 0\n",
    "    hess[1,0] = 0\n",
    "    \n",
    "    step_size = 0.01\n",
    "    loss = obj_func(X_train,y_train,w)\n",
    "    w = w - step_size*np.matmul(output,np.linalg.inv(hm)) \n",
    "    #stopping criteria\n",
    "    if loss - obj_func(X_train,y_train,w) < 0.01:\n",
    "        loss = obj_func(X_train,y_train,w)\n",
    "        break\n",
    "        \n",
    "print(\"Minimized loss (Hessian diagonal approximation):\",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
